\appendix

\chapter{Singular Value Decomposition - SVD}\label{appendix:svd}

Let $\mathbf{M}$ be a real $m \times n$ matrix. The Singular Value Decomposition (SVD) is a factorization that generalizes the eigenvalues decomposition and it states that every matrix can be decomposed in the following form:
\begin{equation}
    \mathbf{M} = \mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}},
    \label{eq:svd}
\end{equation}
where $\mathbf{U}$ is a $m \times m$ matrix, $\mathbf{V}$ is a $n \times n$ matrix and $\Sigma$ is a $m \times n$ positive-definite rectangular diagonal matrix. $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices:
\begin{align}
    \mathbf{U}\mathbf{U}^{\mathsf{T}} = \mathbf{U}^{\mathsf{T}}\mathbf{U} &= \mathbf{I}_{m} \\
    \mathbf{V}\mathbf{V}^{\mathsf{T}} = \mathbf{V}^{\mathsf{T}}\mathbf{V} &= \mathbf{I}_{n}. 
\end{align}

Manipulating the equation \eqref{eq:svd} and using the matrices properties we obtain
\begin{align}
    \mathbf{M}\mathbf{M}^{\mathsf{T}} &= \mathbf{U} \Sigma^2 \mathbf{U}^{\mathsf{T}} \\
    \mathbf{M}^{\mathsf{T}}\mathbf{M} &=  \mathbf{V} \Sigma^2 \mathbf{V}^{\mathsf{T}}.
\end{align}

From this, we observe that the diagonal elements of $\Sigma^2$ are eigenvalues of $\mathbf{M}\mathbf{M}^{\mathsf{T}}$ and $\mathbf{M}\mathbf{M}^{\mathsf{T}}$, which are called row-wise correlation and column-wise correlation matrices, respectively. Since $\mathbf{U}^{\mathsf{T}} = \mathbf{U}^{-1}$ and $\mathbf{V}^{\mathsf{T}} = \mathbf{V}^{-1}$, the columns of $\mathbf{U}$ are the eigenvectors of $\mathbf{M}\mathbf{M}^{\mathsf{T}}$ and the columns of $\mathbf{V}$ are the eigenvectors of $\mathbf{M}^{\mathsf{T}}\mathbf{M}$.

The diagonal elements $\sigma_i := \Sigma_{ii} \geq 0$ are called singular values. The SVD of a matrix is not unique but a default choice is to arrange the decomposition in such a way that the singular values are sorted in descending order, $\sigma_i \geq \sigma_j$ for $i < j$. The number of non-zero singular values is exactly the rank of matrix $\mathbf{M}$. Thus, the SVD of a rank deficient matrix will result in zero or numerically very small singular values. 

A common procedure to avoid degeneracies in the calculations is the singular values selection. This can be done by setting explicitly the unwanted singular values to zero. This can be done in a more insightful way by defining a minimum threshold $\Delta$ and setting to zero the singular values that satisfy $\dfrac{\sigma_i}{\mathrm{max}\left(\sigma_i\right)} < \Delta$. This methods eliminates the less important directions defined by the columns of $\mathbf{U}$ and $\mathbf{V}$ as compared to the direction with higher singular values. 

Since $\Sigma$ is a diagonal matrix, its inverse is obtained simply by $\Sigma^{-1}_{ii} = 1/\sigma_{i}$. For the case that $\sigma_{i} = 0$, one can define $\Sigma^{-1}_{ii} = 0$. Hence, the matrix $n \times m$
\begin{equation}
    \mathbf{M}^{-1} = \mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}},
    \label{eq:svd_inverse}
\end{equation}
is the pseudo-inverse of $\mathbf{M}$ as can be checked by
\begin{align}
    \mathbf{M}^{-1}\mathbf{M} =  \left(\mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}} \right)\left(\mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}}\right) &= \mathbf{I}_{n} \\
    \mathbf{M}\mathbf{M}^{-1} =  \left(\mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}}\right)\left( \mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}} \right)&= \mathbf{I}_{m}. 
\end{align}

The matrix $\mathbf{M}^{-1}$ is also known as Moore-Penrose pseudo-inverse \cite{numerical_recipes}.

A useful version of SVD is the so-called ``economy SVD''. Let $r = \mathrm{min}\left(m, n\right)$, then one can observe that $\Sigma_{ij} = 0$ for $i > r$ ou $j > r$. In this way, all these zero rows or columns in the rectangular $m \times n$ matrix $\Sigma$ can be removed to build a smaller square matrix $\hat{\Sigma}$ with dimension $r \times r$. Doing that allows for reducing the dimension of $\mathbf{U}$ as well, obtaining a rectangular $m \times r$ matrix $\hat{\mathbf{U}}$. The matrix $\mathbf{V}$ is unchanged. In this version, the new matrix $\hat{\mathbf{U}}$ is semi-orthogonal, i.e., $\hat{\mathbf{U}}^{\mathsf{T}}\hat{\mathbf{U}} = \mathbf{I}_r$ but $\hat{\mathbf{U}}\hat{\mathbf{U}}^{\mathsf{T}} \neq \mathbf{I}_m$ in general. The economy SVD is very interesting for numerical purposes, since it is common that $m \gg n$ or $m \ll n$, then using only the minimum useful data contained in the SVD matrices is very computationally beneficial.

The \gls{svd} pseudo-inversion is a powerful tool to solve generic linear systems of equations given by
\begin{equation}
    \mathbf{A} \vec{x} = \vec{b},
    \label{eq:linear_system}
\end{equation}
where $\mathbf{A}$ is a $m \times n$ matrix, $\vec{x}$ a $n \times 1$ column vector and $\vec{b}$ a $m \times 1$ column vector. The case that $m=n$ may be exactly solvable, if $\mathrm{det}\left(\mathbf{A}\right) \neq 0$ the exact solution is obtained by normal inversion. Other two cases that may not have an exact solution occur:
\begin{itemize}
    \item Underdeterminated: $m < n$ and the system has infinitely many solutions, given a generic vector $\vec{b}$. The system does not have enough information given by the elements of $\vec{b}$ to obtain the exact unknowns $\vec{x}$. With the pseudo-inversion it is possible to obtain a solution $\vec{x}_s$ to the linear system such that $|\vec{x}_s|^2 = \sum_{i=1}^{n}x^2_{s, i}$ is minimized. This is called the minimum-norm solution.
    \item Overdeterminated: $m > n$ and the system has no solution, given a generic vector $\vec{b}$. The system has more equations than unknowns, so the system is overconstrained and it is not possible to satisfy exactly and simultaneously all the equations. In this case, with the pseudo-inversion it is obtained an approximate solution $\vec{x}_s$ that minimizes the difference $|\mathbf{A}\vec{x}_s - \vec{b}|$. This is called the least squares solution.
\end{itemize}

The idea in the LOCO algorithm is applying linear approximations to convert a least squared minimization problem in a linear algebra problem, obtaining a linear system of equations. In this process there is much more data points than unkowns to be determined, thus the LOCO algorithm is characterized as an overdetermined problem.

\chapter{BPMs and Correctors Gains}\label{appendix:gains}

The starting point to obtain the LOCO jacobian matrix for the BPMs gains and rolls is the linear transformation:
\begin{equation}
    \vec{u}_{i, \mathrm{real}} = \mathbf{R}^{\mathrm{BPM}}\left(\alpha_i\right) \mathbf{G}_{i}^{\mathrm{BPM}} \vec{u}_{i, \mathrm{meas.}}.
    \label{eq:gain_bpm_app}
\end{equation}

If the orbit vector is viewed as a function $\vec{u}_i = \vec{u}_i\left(\theta_j, \alpha_i, g_{i, x}, g_{i, y}\right)$, the \gls{orm} is $\vec{M}_{ij} = \dfrac{\partial \vec{u}_i}{\partial \theta_j}$. The vector is just a notation to use $\vec{u}_i = (x_i, y_i)$ and $\vec{M}_{ij} = \left(M_{ij}^x, M_{ij}^y\right)$. Moreover, the matrices $\mathbf{R}\left(\alpha_i\right)$ and $\mathbf{G}_{i}^{\mathrm{BPM}}$ satisfies
\begin{equation*}
\dfrac{\partial \mathbf{R}^\mathrm{BPM}\left(\alpha_i\right)}{ \partial  \theta_j} = \dfrac{\partial \mathbf{G}_{i}^{\mathrm{BPM}}}{ \partial \theta_j} = 0.
\end{equation*}

Thus it is possible to obtain from~\eqref{eq:gain_bpm_app} that
\begin{equation}
    \vec{M}_{ij}^{\mathrm{real}} = \mathbf{R}^\mathrm{BPM}\left(\alpha_i\right) \mathbf{G}_{i}^{\mathrm{BPM}} \vec{M}_{ij}^{\mathrm{meas.}}.
    \label{eq:gain_bpm_orm}
\end{equation}

The transformation represented in equation~\eqref{eq:gain_bpm_orm} must be applied in the measured \gls{orm} as the LOCO algorithm updates the values of $\left(\alpha_i, g_{x, i}, g_{y, i}\right)$. 

The residue vector is defined as $\vec{V} = \mathrm{vec}\left(\mathbf{M}^{\mathrm{meas.}} - \mathbf{M}^{\mathrm{model}}\right)$. Since $\mathbf{M}^{\mathrm{measured}}$ must be corrected by the transformation that includes BPM gains and rolls, the new residue vector is elements are 
\begin{equation}
    {V}_k^{\mathrm{real}} = \mathbf{R}^\mathrm{BPM}\left(\alpha_i\right) \mathbf{G}_{i}^{\mathrm{BPM}} {M}^{\mathrm{meas.}}_{ij} - \mathbf{M}^{\mathrm{model}}_{ij},
\end{equation}
where the index $k$ is obtained from $i$ and $j$ by the vectorization. 

To calculate the LOCO jacobian matrix one needs to calculate the derivatives of $\vec{V}^{\mathrm{real}}$ relative to the fit parameters $\left(\alpha_i, g_{x, i}, g_{y, i}\right)$, which are assumed to be independent parameters, obtaining:
\begin{align}
    J_{kl}^{\mathrm{BPMroll}} = \dfrac{\partial {V}_k^{\mathrm{real}}}{\partial \alpha_l} &= \delta_{il}\dfrac{\dif \mathbf{R}^\mathrm{BPM}\left(\alpha_i\right)}{\dif \alpha_l} \mathbf{G}_{i}^{\mathrm{BPM}} {M}^{\mathrm{meas.}}_{ij}, \\
    J_{kl}^{\mathrm{BPMgain}} = \dfrac{\partial {V}_k^{\mathrm{real}}}{\partial g_{l, u}} &= \delta_{il}\mathbf{R}^\mathrm{BPM}\left(\alpha_i\right){M}^{\mathrm{meas.}}_{ij},
\end{align}
where $\delta_{il}$ is the Kronecker delta and 
\begin{equation*}
    \dfrac{\dif \mathbf{R}^\mathrm{BPM}\left(\alpha_i\right)}{\dif \alpha_i} =
    \begin{bmatrix}
    -\sin\alpha_i & \cos\alpha_i \\
     -\cos\alpha_i & -\sin\alpha_i 
    \end{bmatrix}.
\end{equation*}

Due to the sorting used in the \gls{orm} that the horizontal measurements are in the upper blocks and the vertical measurements are in the lower blocks, the transformation matrices of gains and rolls are reorganized as
\begin{align*}
    \mathbf{R}^\mathrm{BPM}_{\alpha} &=
    \begin{bmatrix}
    \mathbf{C}^\alpha &  \mathbf{S}^\alpha \\
     -\mathbf{S}^\alpha & \mathbf{C}^\alpha
    \end{bmatrix}, \\
    \dfrac{\dif \mathbf{R}^\mathrm{BPM}_{\alpha}}{\dif \alpha} &=
    \begin{bmatrix}
    -\mathbf{S}^\alpha &  \mathbf{C}^\alpha \\
    -\mathbf{C}^\alpha & -\mathbf{S}^\alpha
    \end{bmatrix}, \\
    \mathbf{G}^{\mathrm{BPM}} &=
    \begin{bmatrix}
    \mathbf{G}^x & \mathbf{G}^y \\
    \mathbf{G}^x & \mathbf{G}^y
    \end{bmatrix},
\end{align*}
formed with diagonal sub-matrices ${C}^{\alpha}_{ii} = \cos\alpha_i$, ${S}^{\alpha}_{ii} = \sin\alpha_i$, ${G}^{x}_{ii} = g_{i, x}$, ${G}^{y}_{ii} = g_{i, y}$. The transformation in this form is very useful to be applied directly in the measured \gls{orm} and to calculate the jacobian matrix elements:
\begin{align}
    \mathbf{M}^{\mathrm{real}} &= \mathbf{R}^\mathrm{BPM}_\alpha\mathbf{G}^{\mathrm{BPM}} \mathbf{M}^{\mathrm{meas.}}, \\
    J_{kl}^{\mathrm{BPMroll}} &= \delta_{il}\left(\dfrac{\dif \mathbf{R}^\mathrm{BPM}_{\alpha}}{\dif \alpha}\mathbf{G}^{\mathrm{BPM}} \mathbf{M}^{\mathrm{meas.}}\right)_{ij}, \\
    J_{kl}^{\mathrm{BPMgain}} &= \delta_{il}\left(\mathbf{R}^\mathrm{BPM}_\alpha \mathbf{M}^{\mathrm{meas.}}\right)_{ij},
\end{align}
the index $i$ is related to BPM index (rows of \gls{orm}) and it is used to be compared with the index $l$ of the jacobian matrix columns. Again, $i$ and $j$ are converted by vectorization to obtain the index $k$.

For the steering magnets gain the analysis is straightforward. The transformation is
\begin{equation}
    \theta_{j, \mathrm{applied}}^u = g_{j, u}^{\mathrm{corr}}\theta_{j, \mathrm{real}}^u,
    \label{eq:app-corr_gain}
\end{equation}
where the ``applied'' sub-index is the equivalent for the ``measured'' in BPMs.

This transformation can be also cast in a matrix form, with diagonal gain matrices. However, since the kicks are in the denominator of the \gls{orm} with $M_{ij} = \dfrac{\Delta u_i}{\Delta \theta_j}$, the correct way to implement the corrector gain transformation in the \gls{orm} is by its inverse
\begin{equation}
        \mathbf{M}^{\mathrm{real}} = \mathbf{G}^{-1}_{\mathrm{corr}} \mathbf{M}^{\mathrm{meas.}}.
\end{equation}

Since the steering magnet gain was defined by equation~\eqref{eq:app-corr_gain}, the diagonal elements of $\mathbf{G}_{\mathrm{corr}}$ are $G^{\mathrm{corr}}_{ii} = 1/g_{i}$ so the inverse elements are $g_{i}$. This is convenient to obtain the jacobian matrix elements in a linear form, in the same manner that was obtained for the BPMs gains:
\begin{equation}
    J_{kl}^{\mathrm{corr}-\mathrm{gain}} = \delta_{jl}M^{\mathrm{meas.}}_{ij}.
\end{equation}

The index $j$ is related to correctors index (columns of \gls{orm}) and it is used to be compared with the index $l$ of the jacobian matrix columns. Once again, $i$ and $j$ are converted by vectorization to obtain the index $k$. Since the correctors is sorted column-wise in the \gls{orm}, the related gains blocks are organized as $\mathbf{G}_{\mathrm{corr}} =
    \begin{bmatrix}
    \mathbf{G}^x & \mathbf{G}^x \\
    \mathbf{G}^y & \mathbf{G}^y
    \end{bmatrix}$.

If the corrector gain was defined alternatively as $\theta_{j, \mathrm{real}}^u = g_{j, u}^{\mathrm{corr}}\theta_{j, \mathrm{applied}}^u$, the jacobian matrix would contain non-linear elements like $-1/g_{i}^2$, obtained from the derivative of $1/g_{i}$.

The final transformation, containing the BPMs and correctors gains and also the BPM roll is
\begin{equation}
    \mathbf{M}^{\mathrm{real}} = \mathbf{R}^\mathrm{BPM}_\alpha\mathbf{G}^{\mathrm{BPM}} \mathbf{M}^{\mathrm{meas.}}\mathbf{G}^{-1}_{\mathrm{corr}}.
    \label{eq:full_transf}
\end{equation}

In each iteration of the LOCO algorithm, the gains and rolls parameters are updated and the transformation described in equation~\eqref{eq:full_transf} must be applied.

The matrix multiplication order is important, since the \gls{orm} dimension is $2\mathrm{N}_{\mathrm{BPM}} \times \mathrm{N}_{\mathrm{corr}}$, the BPM-related matrices dimensions are $2\mathrm{N}_{\mathrm{BPM}} \times 2\mathrm{N}_{\mathrm{BPM}}$ and the correctors-related matrix dimension is $\mathrm{N}_{\mathrm{corr}} \times \mathrm{N}_{\mathrm{corr}}$. 

\chapter{Integral of Beta in Quadrupoles}
The tune shift caused by a gradient error distribution $k(s)$ is, form Eq.~\eqref{eq:tune_shift}:
\begin{align}
    \Delta\nu_u &= \frac{1}{4\pi} \oint \beta_u(s) k(s) \mathrm{d}s.
\end{align}

A gradient error distributed along a quadrupole can be modeled as
\begin{equation}
    k(s) = 
\begin{cases}
\Delta K \hspace{0.2cm} \text{for} \hspace{0.1cm} s \in [0, L], \\
0 \hspace{0.2cm} \text{for} \hspace{0.1cm} s \notin [0, L]
\end{cases},
\end{equation}
where $L$ is the quadrupole length. The tune shift in this case is
\begin{align}
    \Delta\nu_u &= \frac{\Delta K}{4\pi} \int_{0}^{L} \beta_u(s)\mathrm{d}s.
\end{align}

If the quadrupole strength is intentionally changed by the amount $\Delta K$ and the corresponding tune shift is measured with the beam, the integral of beta function in the quadrupole can be calculated:
\begin{align}
\int_{0}^{L} \beta_u(s) \mathrm{d}s &= 4\pi\frac{\Delta \nu_u}{\Delta K}.
\end{align}

Typically it is convenient to use the integrate quadrupole strength $\Delta KL$, in this way, the quantity that is calculated is the integral of beta function along the quadrupole normalized by the quadrupole length:
\begin{align}
\dfrac{1}{L}\int_{0}^{L} \beta_u(s) \mathrm{d}s &= 4\pi\frac{\Delta \nu_u}{\Delta KL}.
\end{align}

In the approximated case when the beta function is considered constant along the quadrupole, we obtain that $\beta_u(s_q) = 4\pi \dfrac{\Delta \nu_u}{\Delta KL}$, where $s_q$ is the varied quadrupole longitudinal position.

It is important to notice that considering $\Delta KL > 0$ increases the focusing forces in the horizontal plane, thus necessarily decreases the focusing forces in the vertical plane. With this strength change we will obtain $\Delta \nu_x > 0$ and $\Delta \nu_y < 0$. Since the beta function is always positive, in this case the $\Delta KL$ sign must be negative for the vertical betatron function calculation to cancel the $\Delta \nu_y$ sign.

The tune-shift approach is used for the measurement of the integral of betatron functions in the quadrupoles in a storage ring.

To calculate the same quantities in the storage ring model there are a numerical and an analytical approach. The numerical approach basically reproduces the measurement procedure performed in the real storage ring. It may include non-linear effects due to the gradient variation but it also has the disadvantage of computing time. For every quadrupole in the storage ring, the Twiss functions after the gradient variation must be calculated, depending on the time required to calculate the storage ring Twiss functions and on the number of quadrupoles in the lattice, the calculation time may be an inconvenient for a practical use of this numerical approach.

It is also possible to calculate an analytical expression for the beta integral along the quadrupole. Let the Twiss parameters be $(\beta_0, \alpha_0, \gamma_0)$ at the quadrupole entrance. The beta function in a position $s$ along the quadrupole can be propagated as
\begin{equation}
\beta(s) = \beta_0 C^2(s) - 2\alpha_0 C(s)S(s) + \gamma_0S^2(s),
\end{equation}
where the functions $C(s)$ and $S(s)$ are
\[
C(s) = 
\begin{cases}
\cos(\sqrt{K}s) \hspace{0.2cm} \text{for} \hspace{0.1cm} K > 0, \\
\cosh(\sqrt{|K|}s) \hspace{0.2cm} \text{for} \hspace{0.1cm} K < 0, 
\end{cases}
\]
\[
S(s) = 
\begin{cases}
\frac{1}{\sqrt{K}}\sin(\sqrt{K}s) \hspace{0.2cm} \text{for} \hspace{0.1cm} K > 0, \\
\frac{1}{\sqrt{|K|}}\sinh(\sqrt{|K|}s) \hspace{0.2cm} \text{for} \hspace{0.1cm} K < 0.
\end{cases}
\]

Therefore, calculating the $\beta(s)$ integral, we obtain the following results.

For $K > 0$:
\begin{align*}
    \int_{0}^{L} \beta(s) \mathrm{d}s &= \frac{L}{2}\left(\beta_0 + \gamma_0/K\right) \\
    &+ \frac{\sin\left(2\sqrt{K}L\right)}{4 \sqrt{K}}\left(\beta_0 - \gamma_0/K\right) \\
    &- \frac{\alpha_0}{K}\sin^2\left(\sqrt{K}\right).
\end{align*}

For $K < 0$:
\begin{align*}
    \int_{0}^{L} \beta(s) \mathrm{d}s &= \frac{L}{2}\left(\beta_0 + \gamma_0/K\right)  \\ 
    &+ \frac{\sinh\left(2\sqrt{|K|}L\right)}{4 \sqrt{|K|}}\left(\beta_0 - \gamma_0/K\right) \\
    &- \frac{\alpha_0}{|K|L}\sinh^2\left(\sqrt{|K|}\right).
\end{align*}

It is worth to mention again that $K > 0$ for the $x$ plane corresponds to $K < 0$ for the $y$ plane. 

With the analytical approach, the Twiss parameters calculation is required only once to obtain $(\beta_0, \alpha_0, \gamma_0)$ at the quadrupoles entrances, and with the quadrupole strength $K$ and length $L$, the integrals are calculated with the above presented formulae.

\chapter{Feed-down Effect}\label{appendix:feed-down}
The magnetic fields in a storage ring acting on the electron beam depends on the deviation between the beam position transverse position and the magnet magnetic center. If there this type of deviation is present, for example from magnets misalignment or beam orbit distortions, the so-called feed-down effect takes place. The largest the transverse deviations and the magnetic field strengths in a storage ring, the higher is the feed-down effect that, uncontrolled, spoils the beam dynamics. Since this effect should be mitigated as much as possible, this is one of the main reasons behind the very strict magnets alignment specifications in a storage ring, specially for 4\ts{th} generation machines. With transverse displacements, when the beam reaches a magnet which the main field order is $n$ (for $n>1$), the electrons will also be affected by all fields of order $n-1$. The main contributions to the feed-down effect in a storage ring usually come from quadrupoles and sextupoles. 

The effect for quadrupoles can be derived from the hamiltonian in Eq.~\eqref{transv_hamilton}, where it is assumed that the reference orbit are localized at $x=y=0$. 
\begin{equation}
    H_0 = \dfrac{{x'}^2}{2} + \dfrac{{y'}^2}{2} + \left(K(s)- G^{2}(s)\right)\dfrac{{x}^2}{2} - K(s) \frac{y^2}{2} - G(s) x \delta.
\end{equation}

If the reference orbit is transformed by $x(s) \rightarrow x(s) - x_0(s)$ and $y(s) \rightarrow y(s) - y_0(s)$, the corresponding change in the hamiltonian is
\begin{equation}
    H_0 \rightarrow H_0 - K(s)\left(x_0x - y_0y\right) + \dfrac{K(s)}{2}\left(x_0^2 - y_0^2\right).
\end{equation}

Thus, it can be seen that dipolar contributions both in horizontal and vertical planes appears, whose bending magnitudes are given by $K(s)x_0$ and $K(s)y_0$, respectively. Horizontal displacements in quadrupoles produce additional horizontal bending and the vertical displacement in quadrupoles creates vertical bending in the storage ring. The additional constant terms in the hamiltonian does not contribute to the equations of motion. The additional horizontal bending may perturb the horizontal dispersion function $\eta_x$ and also distort the closed orbit. The vertical contribution creates a vertical dispersion $\eta_y$, which might increase the vertical beam emittance, decrease the light source brightness and disturb the closed orbit as well.

The other important contribution comes from sextupoles. The non-linear transverse hamiltonian in the presence of sextupoles is given by:
\begin{equation}
    H_S = H_0 + \dfrac{S(s)}{6}\left(x^3 - 3xy^2\right),
\end{equation}
where $S(s)$ is the sextupolar function around the storage ring.

Applying the coordinate transformation $x(s) \rightarrow x(s) - x_0(s)$, one can calculate that the related change in the hamiltonian is
\begin{equation}
    H_S \rightarrow H_S - \dfrac{S(s)x_0}{2}\left(x^2 - y^2\right) + \dfrac{S(s)x_0^2}{2}x - \dfrac{S(s)x_0^3}{6}.
\end{equation}

Therefore, horizontal displacements in sextupoles produce additional focusing forces, changing the focusing function by $K(s) \rightarrow K(s) - S(s)x_0(s)$ and perturbing the storage ring linear optics, i.e., betatron and dispersion functions. These deviations in sextupoles also create dipolar contributions but its strength depends on $x^2_0(s)$, so this is a second order effect. Again, the additional constant term does not affect the dynamics.

With the transformation in the vertical plane $y(s) \rightarrow y(s) - y_0(s)$, the change in the non-linear hamiltonian is
\begin{equation}
    H_S \rightarrow H_S + S(s)y_0 xy - \dfrac{S(s)y_0^2}{2}x,
\end{equation}
which allows us to conclude that vertical deviations in sextupole produce coupled terms that add skew gradients in the storage ring, introducing perturbations in the lattice related to the transverse betatron coupling. Once again, there is a dipolar perturbation, which depends on $y^2_0(s)$ and is typically less important.

