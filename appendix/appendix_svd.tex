\chapter{Singular Value Decomposition - SVD}\label{appendix:svd}

Let $\mathbf{M}$ be a real $m \times n$ matrix. The Singular Value Decomposition (SVD) is a factorization that generalizes the eigenvalues decomposition and it states that every matrix can be decomposed in the following form:
\begin{equation}
    \mathbf{M} = \mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}},
    \label{eq:svd}
\end{equation}
where $\mathbf{U}$ is a $m \times m$ matrix, $\mathbf{V}$ is a $n \times n$ matrix and $\Sigma$ is a $m \times n$ positive-definite rectangular diagonal matrix. $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices:
\begin{align}
    \mathbf{U}\mathbf{U}^{\mathsf{T}} = \mathbf{U}^{\mathsf{T}}\mathbf{U} &= \mathbf{I}_{m} \\
    \mathbf{V}\mathbf{V}^{\mathsf{T}} = \mathbf{V}^{\mathsf{T}}\mathbf{V} &= \mathbf{I}_{n}. 
\end{align}

Manipulating the equation \eqref{eq:svd} and using the matrices properties we obtain
\begin{align}
    \mathbf{M}\mathbf{M}^{\mathsf{T}} &= \mathbf{U} \Sigma^2 \mathbf{U}^{\mathsf{T}} \\
    \mathbf{M}^{\mathsf{T}}\mathbf{M} &=  \mathbf{V} \Sigma^2 \mathbf{V}^{\mathsf{T}}.
\end{align}

From this, we observe that the diagonal elements of $\Sigma^2$ are eigenvalues of $\mathbf{M}\mathbf{M}^{\mathsf{T}}$ and $\mathbf{M}^{\mathsf{T}}\mathbf{M}$, which are called row-wise correlation and column-wise correlation matrices, respectively. Since $\mathbf{U}^{\mathsf{T}} = \mathbf{U}^{-1}$ and $\mathbf{V}^{\mathsf{T}} = \mathbf{V}^{-1}$, the columns of $\mathbf{U}$ are the eigenvectors of $\mathbf{M}\mathbf{M}^{\mathsf{T}}$ and the columns of $\mathbf{V}$ are the eigenvectors of $\mathbf{M}^{\mathsf{T}}\mathbf{M}$.

The diagonal elements $\sigma_i := \Sigma_{ii} \geq 0$ are called singular values. The SVD of a matrix is unique up to permutations and a default choice is to arrange the decomposition in such a way that the singular values are sorted in descending order, $\sigma_i \geq \sigma_j$ for $i < j$. The number of non-zero singular values is exactly the rank of matrix $\mathbf{M}$. Thus, the SVD of a rank deficient matrix will result in zero or numerically very small singular values. 

A common procedure to avoid degeneracies in the calculations is the singular values selection. This can be done by setting explicitly the unwanted singular values to zero. This can be done in a more insightful way by defining a minimum threshold $\Delta$ and setting to zero the singular values that satisfy $\dfrac{\sigma_i}{\mathrm{max}\left(\sigma_i\right)} < \Delta$. This methods eliminates the less important directions defined by the columns of $\mathbf{U}$ and $\mathbf{V}$ as compared to the direction with higher singular values. 

Since $\Sigma$ is a diagonal matrix, its inverse is obtained simply by $\Sigma^{-1}_{ii} = 1/\sigma_{i}$. For the case that $\sigma_{i} = 0$, one can define $\Sigma^{-1}_{ii} = 0$. Hence, the matrix $n \times m$
\begin{equation}
    \mathbf{M}^{-1} = \mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}},
    \label{eq:svd_inverse}
\end{equation}
is a pseudo-inverse of $\mathbf{M}$ as can be checked by
\begin{align}
    \mathbf{M}^{-1}\mathbf{M} =  \left(\mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}} \right)\left(\mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}}\right) &= \mathbf{I}_{n} \\
    \mathbf{M}\mathbf{M}^{-1} =  \left(\mathbf{U} \Sigma \mathbf{V}^{\mathsf{T}}\right)\left( \mathbf{V} \Sigma^{-1} \mathbf{U}^{\mathsf{T}} \right)&= \mathbf{I}_{m}. 
\end{align}

The matrix $\mathbf{M}^{-1}$ is also known as Moore-Penrose pseudo-inverse \cite{numerical_recipes}.

A useful version of SVD is the so-called ``economy SVD''. Let $r = \mathrm{min}\left(m, n\right)$, then one can observe that $\Sigma_{ij} = 0$ for $i > r$ or $j > r$. In this way, all these zero rows or columns in the rectangular $m \times n$ matrix $\Sigma$ can be removed to build a smaller square matrix $\hat{\Sigma}$ with dimension $r \times r$. If $m > n$, then $r = n$ and this allows for reducing the dimension of $\mathbf{U}$ as well, obtaining a rectangular $m \times r$ matrix $\hat{\mathbf{U}}$ and the matrix $\mathbf{V}$ is unchanged. If $m < n$, then $r=m$ and the transpose case occurs, so $\mathbf{V}$ can be reduced to a $r \times n$ matrix and $\mathbf{U}$ is unchanged. Let's assume $m > n$, which is the case for the matrices in this work. In this situation, the new matrix $\hat{\mathbf{U}}$ is semi-orthogonal, i.e., $\hat{\mathbf{U}}^{\mathsf{T}}\hat{\mathbf{U}} = \mathbf{I}_r$ but $\hat{\mathbf{U}}\hat{\mathbf{U}}^{\mathsf{T}} \neq \mathbf{I}_m$ in general. The economy SVD is very interesting for numerical purposes, since it is common that $m \gg n$ or $m \ll n$, then using only the minimum useful data contained in the SVD matrices is very computationally beneficial.

The \gls{svd} pseudo-inversion is a powerful tool to solve generic linear systems of equations given by
\begin{equation}
    \mathbf{A} \vec{x} = \vec{b},
    \label{eq:linear_system}
\end{equation}
where $\mathbf{A}$ is a $m \times n$ matrix, $\vec{x}$ a $n \times 1$ column vector and $\vec{b}$ a $m \times 1$ column vector. The case that $m=n$ may be exactly solvable, if $\mathrm{det}\left(\mathbf{A}\right) \neq 0$ the exact solution is obtained by normal inversion. Other two cases that may not have an exact solution occur:
\begin{itemize}
    \item Underdeterminated: $m < n$ and the system has infinitely many solutions, given a generic vector $\vec{b}$. The system does not have enough information given by the elements of $\vec{b}$ to obtain the exact unknowns $\vec{x}$. With the pseudo-inversion it is possible to obtain a solution $\vec{x}_s$ to the linear system such that $|\vec{x}_s|^2 = \sum_{i=1}^{n}x^2_{s, i}$ is minimized. This is called the minimum-norm solution.
    \item Overdeterminated: $m > n$ and the system has no solution, given a generic vector $\vec{b}$. The system has more equations than unknowns, so the system is overconstrained and it is not possible to satisfy exactly and simultaneously all the equations. In this case, with the pseudo-inversion it is obtained an approximate solution $\vec{x}_s$ that minimizes the difference $|\mathbf{A}\vec{x}_s - \vec{b}|$. This is called the least squares solution.
\end{itemize}

The idea in LOCO algorithm is applying linear approximations to convert a least squared minimization problem in a linear algebra problem, obtaining a linear system of equations. In this process there is much more data points than unkowns to be determined, thus LOCO algorithm is characterized as an overdetermined problem.